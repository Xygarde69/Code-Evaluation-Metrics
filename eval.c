/**
 * @file enhanced_safe_eval.c
 * @brief Enhanced evaluator that uses LLM-generated test cases
 * 
 * This version reads test cases from JSON generated by the LLM analyzer
 * instead of using hardcoded test cases.
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/wait.h>
#include <fcntl.h>
#include <signal.h>
#include <errno.h>
#include <time.h>
#include <sys/stat.h>
#include <sys/resource.h>
#include <ctype.h>
#include <json-c/json.h>  // For JSON parsing

// --- Configuration & Constants ---
#define MAX_TESTS 20  // Increased for LLM-generated tests
#define TIMEOUT_SECONDS 5
#define MAX_OUTPUT_SIZE 4096
#define MEMORY_LIMIT_MB 64
#define CPU_TIME_LIMIT_S 10
#define MAX_INPUT_SIZE 1024
#define MAX_EXPECTED_OUTPUT_SIZE 1024
#define MAX_DESCRIPTION_SIZE 256

#define RESULTS_JSON_PATH "/tmp/eval_results.json"
#define EXEC_FAILURE_EXIT_CODE 127
#define ROBUSTNESS_SIGINT_WAIT_US 200000

// --- Enhanced Structs ---
typedef struct {
    char input[MAX_INPUT_SIZE];
    char expected_output[MAX_EXPECTED_OUTPUT_SIZE];
    char description[MAX_DESCRIPTION_SIZE];
    char category[32];  // normal, edge, error, corner
    float weight;       // Test importance weight
} DynamicTestCase;

typedef struct {
    DynamicTestCase tests[MAX_TESTS];
    int num_tests;
    char program_description[512];
    char program_type[64];
    char difficulty_level[32];
    char potential_edge_cases[MAX_TESTS][256];
    int num_edge_cases;
} TestSuite;

typedef struct {
    float passrate;
    float memory_score;
    float robustness_score;
    float weighted_score;  // New: weight-based scoring
    long execution_time_ms;
    int tests_passed;
    int tests_failed;
    char failed_tests[MAX_TESTS][512];  // Details of failed tests
    int num_failed_details;
} EnhancedEvalMetrics;

// --- Global State ---
char executable_path[256];
char temp_dir_path[256];
TestSuite test_suite;

// --- Function Prototypes ---
void cleanup(void);
void handle_signal(int sig);
long current_time_ms(void);
void set_child_resource_limits(void);
int compile_source(const char *source_filename);
int run_test_process(const char *input, char *output_buffer, size_t buffer_size);
int load_test_cases_from_json(const char *json_file);
float calculate_dynamic_passrate(EnhancedEvalMetrics *metrics);
float analyze_memory(void);
float check_robustness(void);
void write_enhanced_results_to_json(const EnhancedEvalMetrics *metrics);
void trim_trailing_whitespace(char *str);
void print_test_suite_info(void);

// --- JSON Loading Functions ---

/**
 * @brief Loads test cases from LLM-generated JSON file
 */
int load_test_cases_from_json(const char *json_file) {
    FILE *file = fopen(json_file, "r");
    if (!file) {
        fprintf(stderr, "❌ Cannot open test cases file: %s\n", json_file);
        return -1;
    }

    // Read entire file
    fseek(file, 0, SEEK_END);
    long file_size = ftell(file);
    fseek(file, 0, SEEK_SET);
    
    char *json_string = malloc(file_size + 1);
    fread(json_string, 1, file_size, file);
    json_string[file_size] = '\0';
    fclose(file);

    // Parse JSON
    json_object *root = json_tokener_parse(json_string);
    if (!root) {
        fprintf(stderr, "❌ Invalid JSON in test cases file\n");
        free(json_string);
        return -1;
    }

    // Extract program metadata
    json_object *desc_obj, *type_obj, *diff_obj, *tests_obj;
    
    if (json_object_object_get_ex(root, "program_description", &desc_obj)) {
        strncpy(test_suite.program_description, json_object_get_string(desc_obj), 
                sizeof(test_suite.program_description) - 1);
    }
    
    if (json_object_object_get_ex(root, "program_type", &type_obj)) {
        strncpy(test_suite.program_type, json_object_get_string(type_obj), 
                sizeof(test_suite.program_type) - 1);
    }
    
    if (json_object_object_get_ex(root, "difficulty_level", &diff_obj)) {
        strncpy(test_suite.difficulty_level, json_object_get_string(diff_obj), 
                sizeof(test_suite.difficulty_level) - 1);
    }

    // Extract test cases
    if (!json_object_object_get_ex(root, "test_cases", &tests_obj)) {
        fprintf(stderr, "❌ No test_cases found in JSON\n");
        json_object_put(root);
        free(json_string);
        return -1;
    }

    int array_len = json_object_array_length(tests_obj);
    test_suite.num_tests = (array_len > MAX_TESTS) ? MAX_TESTS : array_len;

    for (int i = 0; i < test_suite.num_tests; i++) {
        json_object *test_obj = json_object_array_get_idx(tests_obj, i);
        json_object *input_obj, *output_obj, *desc_obj, *cat_obj, *weight_obj;

        if (json_object_object_get_ex(test_obj, "input", &input_obj)) {
            strncpy(test_suite.tests[i].input, json_object_get_string(input_obj), 
                    sizeof(test_suite.tests[i].input) - 1);
        }

        if (json_object_object_get_ex(test_obj, "expected_output", &output_obj)) {
            strncpy(test_suite.tests[i].expected_output, json_object_get_string(output_obj), 
                    sizeof(test_suite.tests[i].expected_output) - 1);
        }

        if (json_object_object_get_ex(test_obj, "description", &desc_obj)) {
            strncpy(test_suite.tests[i].description, json_object_get_string(desc_obj), 
                    sizeof(test_suite.tests[i].description) - 1);
        }

        if (json_object_object_get_ex(test_obj, "category", &cat_obj)) {
            strncpy(test_suite.tests[i].category, json_object_get_string(cat_obj), 
                    sizeof(test_suite.tests[i].category) - 1);
        }

        if (json_object_object_get_ex(test_obj, "weight", &weight_obj)) {
            test_suite.tests[i].weight = json_object_get_double(weight_obj);
        } else {
            test_suite.tests[i].weight = 1.0;  // Default weight
        }
    }

    // Extract potential edge cases
    json_object *edge_cases_obj;
    if (json_object_object_get_ex(root, "potential_edge_cases", &edge_cases_obj)) {
        int edge_array_len = json_object_array_length(edge_cases_obj);
        test_suite.num_edge_cases = (edge_array_len > MAX_TESTS) ? MAX_TESTS : edge_array_len;
        
        for (int i = 0; i < test_suite.num_edge_cases; i++) {
            json_object *edge_obj = json_object_array_get_idx(edge_cases_obj, i);
            strncpy(test_suite.potential_edge_cases[i], json_object_get_string(edge_obj), 
                    sizeof(test_suite.potential_edge_cases[i]) - 1);
        }
    }

    json_object_put(root);
    free(json_string);
    return 0;
}

/**
 * @brief Enhanced passrate calculation with weighted scoring
 */
float calculate_dynamic_passrate(EnhancedEvalMetrics *metrics) {
    metrics->tests_passed = 0;
    metrics->tests_failed = 0;
    metrics->num_failed_details = 0;
    
    float total_weight = 0.0f;
    float passed_weight = 0.0f;
    
    printf("   Running %d LLM-generated test cases:\n", test_suite.num_tests);
    
    for (int i = 0; i < test_suite.num_tests; i++) {
        char output_buf[MAX_OUTPUT_SIZE];
        total_weight += test_suite.tests[i].weight;
        
        printf("   Test %d [%s]: %s\n", i+1, test_suite.tests[i].category, 
               test_suite.tests[i].description);
        
        if (run_test_process(test_suite.tests[i].input, output_buf, sizeof(output_buf)) == 0) {
            trim_trailing_whitespace(output_buf);
            
            if (strcmp(output_buf, test_suite.tests[i].expected_output) == 0) {
                printf("      ✅ PASS\n");
                metrics->tests_passed++;
                passed_weight += test_suite.tests[i].weight;
            } else {
                printf("      ❌ FAIL - Expected: '%s', Got: '%s'\n", 
                       test_suite.tests[i].expected_output, output_buf);
                metrics->tests_failed++;
                
                // Record failure details
                if (metrics->num_failed_details < MAX_TESTS) {
                    snprintf(metrics->failed_tests[metrics->num_failed_details], 512,
                            "Test %d (%s): Expected '%s', Got '%s'", 
                            i+1, test_suite.tests[i].description,
                            test_suite.tests[i].expected_output, output_buf);
                    metrics->num_failed_details++;
                }
            }
        } else {
            printf("      ❌ FAIL - Timeout or execution error\n");
            metrics->tests_failed++;
            
            if (metrics->num_failed_details < MAX_TESTS) {
                snprintf(metrics->failed_tests[metrics->num_failed_details], 512,
                        "Test %d (%s): Execution timeout or error", 
                        i+1, test_suite.tests[i].description);
                metrics->num_failed_details++;
            }
        }
    }
    
    // Calculate both simple and weighted scores
    float simple_passrate = (float)metrics->tests_passed / test_suite.num_tests * 100.0f;
    metrics->weighted_score = (total_weight > 0) ? (passed_weight / total_weight * 100.0f) : 0.0f;
    
    return simple_passrate;
}

/**
 * @brief Prints information about the loaded test suite
 */
void print_test_suite_info(void) {
    printf("📋 Test Suite Information:\n");
    printf("   Program: %s\n", test_suite.program_description);
    printf("   Type: %s\n", test_suite.program_type);
    printf("   Difficulty: %s\n", test_suite.difficulty_level);
    printf("   Tests: %d test cases loaded\n", test_suite.num_tests);
    
    if (test_suite.num_edge_cases > 0) {
        printf("   Edge Cases to Consider:\n");
        for (int i = 0; i < test_suite.num_edge_cases; i++) {
            printf("     • %s\n", test_suite.potential_edge_cases[i]);
        }
    }
    printf("\n");
}

/**
 * @brief Enhanced results output with detailed failure information
 */
void write_enhanced_results_to_json(const EnhancedEvalMetrics *metrics) {
    FILE *f = fopen(RESULTS_JSON_PATH, "w");
    if (!f) {
        perror("fopen (results.json)");
        return;
    }
    
    fprintf(f, "{\n");
    fprintf(f, "  \"program_description\": \"%s\",\n", test_suite.program_description);
    fprintf(f, "  \"program_type\": \"%s\",\n", test_suite.program_type);
    fprintf(f, "  \"difficulty_level\": \"%s\",\n", test_suite.difficulty_level);
    fprintf(f, "  \"passrate\": %.1f,\n", metrics->passrate);
    fprintf(f, "  \"weighted_score\": %.1f,\n", metrics->weighted_score);
    fprintf(f, "  \"memory_score\": %.1f,\n", metrics->memory_score);
    fprintf(f, "  \"robustness_score\": %.1f,\n", metrics->robustness_score);
    fprintf(f, "  \"tests_passed\": %d,\n", metrics->tests_passed);
    fprintf(f, "  \"tests_failed\": %d,\n", metrics->tests_failed);
    fprintf(f, "  \"total_tests\": %d,\n", test_suite.num_tests);
    fprintf(f, "  \"execution_time_ms\": %ld,\n", metrics->execution_time_ms);
    
    // Include failed test details
    fprintf(f, "  \"failed_test_details\": [\n");
    for (int i = 0; i < metrics->num_failed_details; i++) {
        fprintf(f, "    \"%s\"", metrics->failed_tests[i]);
        if (i < metrics->num_failed_details - 1) fprintf(f, ",");
        fprintf(f, "\n");
    }
    fprintf(f, "  ],\n");
    
    // Include potential edge cases for further analysis
    fprintf(f, "  \"potential_edge_cases\": [\n");
    for (int i = 0; i < test_suite.num_edge_cases; i++) {
        fprintf(f, "    \"%s\"", test_suite.potential_edge_cases[i]);
        if (i < test_suite.num_edge_cases - 1) fprintf(f, ",");
        fprintf(f, "\n");
    }
    fprintf(f, "  ]\n");
    fprintf(f, "}\n");
    fclose(f);
}

// --- Main Logic (Modified) ---

int main(int argc, char **argv) {
    if (argc < 3) {
        fprintf(stderr, "Usage: %s <source.c> <test_cases.json>\n", argv[0]);
        return 1;
    }

    // Set up signal handlers and cleanup routine
    signal(SIGINT, handle_signal);
    signal(SIGTERM, handle_signal);
    atexit(cleanup);

    // Load LLM-generated test cases
    printf("🔍 Loading LLM-generated test cases...\n");
    if (load_test_cases_from_json(argv[2]) != 0) {
        fprintf(stderr, "❌ Failed to load test cases from %s\n", argv[2]);
        return 1;
    }
    
    print_test_suite_info();

    // Create secure temporary directory
    char temp_dir_template[] = "/tmp/safe_eval_XXXXXX";
    if (mkdtemp(temp_dir_template) == NULL) {
        perror("mkdtemp failed");
        return 1;
    }
    snprintf(temp_dir_path, sizeof(temp_dir_path), "%s", temp_dir_template);

    long start_time = current_time_ms();

    printf("1. Compiling source file: %s\n", argv[1]);
    if (compile_source(argv[1]) != 0) {
        fprintf(stderr, "❌ Compilation failed.\n");
        EnhancedEvalMetrics metrics = {0};
        write_enhanced_results_to_json(&metrics);
        return 1;
    }
    printf("   ✅ Compilation successful.\n\n");

    EnhancedEvalMetrics metrics = {0};
    
    printf("2. Running LLM-generated correctness tests...\n");
    metrics.passrate = calculate_dynamic_passrate(&metrics);
    printf("   ✅ Simple Passrate: %.1f%% (%d/%d tests passed)\n", 
           metrics.passrate, metrics.tests_passed, test_suite.num_tests);
    printf("   ✅ Weighted Score: %.1f%%\n\n", metrics.weighted_score);

    printf("3. Analyzing memory usage with Valgrind...\n");
    metrics.memory_score = analyze_memory();
    printf("   ✅ Memory Score: %.1f\n\n", metrics.memory_score);

    printf("4. Checking robustness...\n");
    metrics.robustness_score = check_robustness();
    printf("   ✅ Robustness Score: %.1f\n\n", metrics.robustness_score);

    metrics.execution_time_ms = current_time_ms() - start_time;

    write_enhanced_results_to_json(&metrics);
    printf("🎉 Enhanced evaluation complete. Results written to %s\n", RESULTS_JSON_PATH);
    printf("📊 Ready for Stage 3 analysis...\n");

    return 0;
}

// ... [Keep all the existing utility functions: cleanup, handle_signal, current_time_ms, 
//      set_child_resource_limits, compile_source, run_test_process, analyze_memory, 
//      check_robustness, trim_trailing_whitespace from the original code] ...
